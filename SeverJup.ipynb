{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color=blue>**SEVER: Learning in the Presence of Outliers**</font>\n",
    "===========================\n",
    "---\n",
    "## Gabe Mancino-Ball\n",
    "### CSCI 6971 - R.P.I. - Spring 2019\n",
    "\n",
    "Link to original paper [here](https://arxiv.org/pdf/1803.02815.pdf)\n",
    "\n",
    "Authors: I. Diakonikolas, G. Kamath, D. Kane, J. Li, J. Steinhardt, A.  Stewart\n",
    "\n",
    "Published at **NeurIPS 2018 Workshop on Security in Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem Setting\n",
    "---\n",
    "\n",
    "**Stochastic Optimization**\n",
    "\n",
    "$$\\min_{w\\in\\mathcal{H}}\\bar{f}(w):=\\mathbf{E}_{f\\sim p^*}[f(w)]$$\n",
    "\n",
    "where $p^*$ is a distribution of functions $f:\\mathcal{H}\\to\\mathbf{R}$ and our *goal* is to find $w^*\\in\\mathcal{H}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Caveat\n",
    "---\n",
    "\n",
    ">We have structured **outliers** planted in our data\n",
    "\n",
    "This is reasonable in many settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Assumptions\n",
    "---\n",
    "\n",
    "- $g(\\cdot)$ is **linear** combination of features, i.e. $g(w)=w^\\top x$\n",
    "- $f(\\cdot):=l(g(\\cdot))$, i.e. $f$ is our **loss** function\n",
    "\n",
    "#### This is Empirical Risk Minimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss Functions\n",
    "---\n",
    "\n",
    "#### $l_2$-Loss\n",
    "$$f(w)=\\frac{1}{2}(w^\\top x-y)^2$$\n",
    "for regression\n",
    "\n",
    "#### Hinge Loss\n",
    "\n",
    "$$f(w)=\\max(0,1-y(w^\\top x))$$\n",
    "for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Proposed Solution\n",
    "---\n",
    "\n",
    "\n",
    "<img src='severmap.png' width='90%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Algorithm Motivation\n",
    "---\n",
    "- Fit a model on original training data containing $S$ points to learn $\\hat{w}$\n",
    "- Compute the gradients of $f_{i\\in S}(\\hat{w})$\n",
    "- Compute the **average** gradient $\\hat{\\nabla}:=\\frac{1}{\\lvert S\\rvert}\\sum_{i\\in S}\\nabla f_i(\\hat{w})$\n",
    "- Compare $\\hat{\\nabla}$ to $\\nabla f_{i\\in S}(\\hat{w})$ by forming a matrix $G:=[\\nabla f_i(\\hat{w})-\\hat{\\nabla}]_{i\\in S}$\n",
    "- Remove points that satisfy a certain criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Removal of Points and Outlier Scores\n",
    "---\n",
    "- Compute the SVD of $G$ to reveal space of gradients\n",
    "- If this space is corrupted, some singular values will be much larger than others\n",
    "- Let $v$ be the top right singular vector\n",
    "- If $i^{th}$ **data point** is corrupted, we expect $\\nabla f_i(\\hat{w})-\\hat{\\nabla}$ to be large\n",
    "- Compute $(\\nabla f_i(\\hat{w})-\\hat{\\nabla})^\\top v$\n",
    "- If this is \"large\" this implies that these vectors point in the same direction so remove this data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SEVER Algorithm\n",
    "---\n",
    "\n",
    "**Input:** Sample functions $f_1,\\dots,f_n:\\mathcal{H}\\to\\mathbb{R}$, Learner $\\mathcal{L}$, Parameter $\\sigma>0.$\n",
    "\n",
    "**Initialize:** $S=\\{1,\\dots,n\\}$\n",
    "\n",
    "Compute: $w=\\mathcal{L}(\\{f_i\\}_{i\\in S})$, $\\hat{\\nabla}:=\\frac{1}{\\lvert S\\rvert}\\sum_{i\\in S}\\nabla f_i(w)$, $G:=[\\nabla f_i(w)-\\hat{\\nabla}]_{i\\in S}$, $v$ from SVD of $G$\n",
    "\n",
    "Compute *outlier scores* $\\gamma_i=\\big((\\nabla f_i(w)-\\hat{\\nabla})^\\top v\\big)^2$\n",
    "\n",
    "Filter $S$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Filter Algorithm\n",
    "---\n",
    "Compute $\\sum_i\\gamma_i$ and if this is less than $c\\sigma$ (for some $c>1$) then remove no points and **terminate the algorithm**\n",
    "\n",
    "Otherwise draw $T\\sim UNI(0,\\max_i\\gamma_i)$ and throw out points $j$ such that $\\gamma_j<T$ and **repeat the algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Theoretical Guarantees\n",
    "---\n",
    "We require that **at most** 50% of the data is corrupted\n",
    "\n",
    "- Number of sample points is proportional to dimension of data\n",
    "- `SEVER` terminates in at most $n$ iterations\n",
    "- 9/10 of the time the returned $w^*$ is a critical point of $\\mathbf{E}_{f\\sim p^*}[f(w)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Results\n",
    "---\n",
    "\n",
    "- Test on Ridge Regression Problem\n",
    "- Use artificial data (generated from Gaussian distribution)\n",
    "- Use song prediction data\n",
    "- Use Factorized Regularized Conjugate Gradient to solve Ridge Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attack Data\n",
    "---\n",
    "- If $(x,y)$ are original data, create malicious data as follows:\n",
    "$$x_{bad}=\\frac{1}{\\alpha\\cdot n_{bad}}y^\\top x$$\n",
    "$$y_{bad}=-\\beta$$\n",
    "- Causes Ridge Regression to output $w=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Compared Methods\n",
    "---\n",
    "- `TheorySEVER` (Proposed)\n",
    "- `PracSEVER` (Practical): remove top $p$ percent of outliers for a set number of iterations\n",
    "- `GradCenter`: remove top $p$ percent of outliers$^*$ for a set number of iterations\n",
    "- `CG`\n",
    "\n",
    "$^*$ outliers are measured by $\\lVert\\nabla f_i(w)-\\hat{\\nabla}\\rVert_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import random as rnd\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# For displaying images side by side\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "td {\n",
    "  font-size: 25px\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Solvers for Ridge Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Include solvers for ridge regression case\n",
    "\n",
    "def BlackBoxA(A, X, gam):\n",
    "    '''\n",
    "    Compute (A^tA+gam*I)X\n",
    "\n",
    "    INPUTS:\n",
    "    -------\n",
    "    A = feature matrix\n",
    "    X = weight matrix\n",
    "    gam = regularized parameter\n",
    "\n",
    "    OUTPUTS:\n",
    "    --------\n",
    "    ax = (A^tA+gam*I)X\n",
    "    '''\n",
    "\n",
    "    n = A.shape[1]\n",
    "\n",
    "    ax = np.dot(A, X)\n",
    "    ax = np.dot(np.transpose(A), ax)\n",
    "    ax = ax + np.dot(gam*np.eye(n), X)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def FactorizedRegularizedCG(BlackBoxA, A, Y, gamma, maxIters, eps, x0):\n",
    "    '''\n",
    "    Conjugate Gradient for Solving Linear Ridge Regression\n",
    "\n",
    "    INPUTS:\n",
    "    -------\n",
    "    BlackBoxA = function that computes ~A*x\n",
    "    A = matrix of features (rows are features)\n",
    "    Y = vector of targets\n",
    "    gamma = regularization constant\n",
    "    maxIters = maximum number of iterations\n",
    "    eps = error tolerance\n",
    "    x0 = initial guess for x\n",
    "\n",
    "    OUTPUTS:\n",
    "    --------\n",
    "    X = feature weight matrix\n",
    "    numIters = number of iterations ran as a vector\n",
    "    timeInside = total time spent evaluating the function\n",
    "    '''\n",
    "\n",
    "    tstart = time.time()\n",
    "\n",
    "    [n, d] = A.shape\n",
    "    m = len(Y)\n",
    "\n",
    "    # Conjugate Gradient Algorithm\n",
    "\n",
    "    Y = np.dot(np.transpose(A), Y)\n",
    "\n",
    "    # Initialization\n",
    "    comp = 0\n",
    "    ynorm = la.norm(Y)\n",
    "    X = x0\n",
    "\n",
    "    res = Y - BlackBoxA(A, X, gamma)\n",
    "    p = res\n",
    "\n",
    "    for i in range(maxIters):\n",
    "\n",
    "        numIters = i + 1\n",
    "\n",
    "        ap = BlackBoxA(A, p, gamma)\n",
    "\n",
    "        # Compute alpha step size\n",
    "        num = np.dot(np.transpose(res), res)\n",
    "        den = np.dot(np.transpose(p), ap)\n",
    "        alpha = num / den\n",
    "\n",
    "        # Update iterate and residual\n",
    "        X = X + alpha*p\n",
    "        res = res - alpha*ap\n",
    "\n",
    "        # Check termination criteria\n",
    "        multnorm = la.norm(BlackBoxA(A, X, gamma) - Y, axis=0)\n",
    "        check = multnorm - eps*ynorm\n",
    "        sol = np.less_equal(check, comp).flatten()\n",
    "\n",
    "        if np.sum(sol) == sol.shape[0]:\n",
    "            # print('All targets have converged. We are on iterate:', numIters, '.')\n",
    "            break\n",
    "\n",
    "        # Update conjugate search directions\n",
    "        bnum = np.dot(np.transpose(res), res)\n",
    "        beta = bnum / num\n",
    "        p = res + beta*p\n",
    "\n",
    "    tend = time.time()\n",
    "    timeInside = (tend - tstart)*1000\n",
    "\n",
    "    return [X, numIters, timeInside]\n",
    "\n",
    "\n",
    "def regGrad(x, y, w, lam):\n",
    "    '''\n",
    "    Gradient for Ridge Regression Function\n",
    "    \n",
    "    INPUTS:\n",
    "    -------\n",
    "    x = feature matrix (ith row is feature vector)\n",
    "    y = target vector\n",
    "    w = optimal parameter vector\n",
    "    lam = regularization parameter\n",
    "    \n",
    "    OUTPUTS:\n",
    "    -------\n",
    "    grad = gradient evaluated at w where ith column is grad_i\n",
    "    '''\n",
    "    \n",
    "    lhs = np.diag(np.dot(x, w) - y)\n",
    "    grad = np.dot(lhs, x) + lam*w\n",
    "    grad = np.transpose(grad)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Code for Generating Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data as described in the paper\n",
    "\n",
    "def genRan(n, t, l, w, tog, nbad, alpha, beta):\n",
    "    '''\n",
    "    Generate Random Samples\n",
    "    \n",
    "    INPUTS:\n",
    "    -------\n",
    "    n = number of training samples\n",
    "    t = number of testing samples\n",
    "    l = length of data vector\n",
    "    w = optimal parameter of problem\n",
    "    tog = toggle for whether we want data for regression or classification\n",
    "    (1 for classification, anything else for regression)\n",
    "    nbad = number of attack data\n",
    "    alpha, beta = attack parameters\n",
    "    \n",
    "    OUTPUTS:\n",
    "    -------\n",
    "    x = feature data\n",
    "    y = target data\n",
    "    xt = test feature data\n",
    "    yt = test target data\n",
    "    '''\n",
    "    \n",
    "    # Toggle for classification\n",
    "    if tog == 1:\n",
    "    \n",
    "        # Generate all data\n",
    "        x = np.random.normal(loc=0, scale=1, size=(n+t, l))\n",
    "        z = np.random.normal(loc=0, scale=1, size=n+t)\n",
    "        y = np.dot(x, w) + z\n",
    "    \n",
    "        # Generate synthetic attack points\n",
    "        if nbad > 0:\n",
    "            badsamp = np.array(rnd.sample((range(n+t)), nbad))\n",
    "            x[badsamp] = (1/(nbad*alpha))*np.dot(np.transpose(y[badsamp]), x[badsamp])\n",
    "            y[badsamp] = -beta\n",
    "        \n",
    "        # Change to class\n",
    "        y = np.sign(y)\n",
    "        yt = np.sign(yt)\n",
    "        \n",
    "        # Split into testing and training data\n",
    "        xt = x[n:n+t]\n",
    "        yt = y[n:n+t]\n",
    "        x = x[0:n]\n",
    "        y = y[0:n]\n",
    "    \n",
    "    # Else only contaminate the training set for regression\n",
    "    else:\n",
    "    \n",
    "        # Generate training data\n",
    "        x = np.random.normal(loc=0, scale=1, size=(n, l))\n",
    "        z = np.random.normal(loc=0, scale=1, size=n)\n",
    "        y = np.dot(x, w) + z\n",
    "     \n",
    "        # Generate testing data\n",
    "        xt = np.random.normal(loc=0, scale=1, size=(t, l))\n",
    "        zt = np.random.normal(loc=0, scale=1, size=t)\n",
    "        yt = np.dot(xt, w) + zt\n",
    "    \n",
    "        # Generate synthetic attack data on testing set\n",
    "        if nbad > 0:\n",
    "            badsamp_test = np.array(rnd.sample((range(n)), nbad))\n",
    "            x[badsamp_test] = (1/(nbad*alpha))*np.dot(np.transpose(y[badsamp_test]), x[badsamp_test])\n",
    "            y[badsamp_test] = -beta\n",
    "        \n",
    "    \n",
    "    return [x, y, xt, yt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Code for Corrupting Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def dataAttack(x, y, pb, alpha, beta):\n",
    "    '''\n",
    "    Corrupt a fraction of training data\n",
    "    \n",
    "    INPUTS:\n",
    "    -------\n",
    "    x = feature matrix\n",
    "    y = target vector\n",
    "    pb = percent of samples you want to corrupt\n",
    "    alpha, beta = attack paramters\n",
    "    \n",
    "    OUTPUTS:\n",
    "    --------\n",
    "    xc = corrupted features\n",
    "    yc = corrupted targets\n",
    "    '''\n",
    "    [n, d] = x.shape\n",
    "    \n",
    "    xc = np.copy(x)\n",
    "    yc = np.copy(y)\n",
    "    \n",
    "    nb = math.floor(pb*n)\n",
    "    \n",
    "    if nb > n:\n",
    "        print('Choose less data to corrupt.')\n",
    "        return\n",
    "    \n",
    "    badsamp = np.array(rnd.sample((range(n)), nb))\n",
    "    xc[badsamp] = (1/(nb*alpha))*np.dot(np.transpose(yc[badsamp]), xc[badsamp])\n",
    "    yc[badsamp] = -beta\n",
    "    \n",
    "    return [xc, yc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Gradient Centered Removal of Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def gradRemoval(x, y, FactorizedRegularizedCG, BlackBoxA, regGrad, lam, maxit, p, gam, maxIters, eps, x0):\n",
    "    '''\n",
    "    Gradient Centered Removal of Outliers (Regression)\n",
    "    \n",
    "    INPUTS:\n",
    "    -------\n",
    "    x = feature matrix\n",
    "    y = target vector\n",
    "    FactorizedRegularizedCG = ridge regression solver\n",
    "    BlackBoxA = function for computing LHS in RR problem\n",
    "    regGrad = function for computing gradient of objective\n",
    "    lam = regularization parameter (unscaled)\n",
    "    maxit = max iterations for SEVER\n",
    "    p = fraction of points to throw out through filter\n",
    "    gam = regularization parameter (scaled)\n",
    "    maxIters = maximum iterations for solver\n",
    "    eps = error tolerance for solver\n",
    "    x0 = initial guess for w\n",
    "    \n",
    "    OUTPUTS:\n",
    "    -------\n",
    "    w = optimal parameter after outliers have been filtered\n",
    "    it = iterations of SEVER\n",
    "    t_s = run time\n",
    "    '''\n",
    "    \n",
    "    tstart = time.time()\n",
    "    \n",
    "    cs = x.shape[0]\n",
    "    s = np.array(range(cs))\n",
    "    x1 = x\n",
    "    y1 = y\n",
    "    it = 0\n",
    "    \n",
    "    # Find optimal solution using all data\n",
    "    [w, iters, t] = FactorizedRegularizedCG(BlackBoxA, x, y, gam, maxIters, eps, x0)\n",
    "    \n",
    "    while it < maxit:\n",
    "        \n",
    "        # Extract new indices\n",
    "        x = x1[s, :]\n",
    "        y = y1[s]\n",
    "        \n",
    "        # Find optimal solution from set of s points\n",
    "        [w, iters, t] = FactorizedRegularizedCG(BlackBoxA, x, y, gam, maxIters, eps, w)\n",
    "        \n",
    "        # Compute the gradient at s points\n",
    "        gradmat = regGrad(x, y, w, lam)\n",
    "        \n",
    "        # Form G matrix which is gradient at each point minus the average\n",
    "        # G has s rows (number of used indices) and d columns (dimension of data)\n",
    "        # Thus each row of G is grad_i-ave_grad\n",
    "        gradhat = (1/cs)*np.sum(gradmat, axis=1)\n",
    "        g = gradmat - np.array([gradhat,]*cs).transpose()\n",
    "        g = g.transpose()\n",
    "        \n",
    "        # Compute outlier scores\n",
    "        grem = la.norm(g, axis=1)\n",
    "        grem = grem.flatten()\n",
    "        \n",
    "        # Compute fraction of outliers needed to discard\n",
    "        n = len(grem)\n",
    "        frac = math.floor(p*n)\n",
    "    \n",
    "        # Sort tao smallest to largest value\n",
    "        # This returns indices\n",
    "        grem = np.argsort(grem)\n",
    "    \n",
    "        # Discard high values\n",
    "        ind = grem[0:(n - frac)]\n",
    "    \n",
    "        # Take indices from previous candidate set\n",
    "        s = s[ind]\n",
    "        \n",
    "        cs = len(s)\n",
    "        \n",
    "        if cs == 0:\n",
    "            print('You have chosen too many outliers to remove at each iteration.')\n",
    "            break\n",
    "        \n",
    "        # Increase iteration count\n",
    "        it = it + 1\n",
    "    \n",
    "    # Report run time of SEVER in milliseconds\n",
    "    tend = time.time()\n",
    "    t_s = (tend - tstart)*1000\n",
    "        \n",
    "    return [w, it, t_s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Proposed SEVER (theory supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def SEVERfilt(s, tao, sigma, c):\n",
    "    '''\n",
    "    Proposed filter function for sever.\n",
    "    \n",
    "    INPUTS:\n",
    "    -------\n",
    "    s = set of indices\n",
    "    tao = vector of outlier scores\n",
    "    sigma = variance parameter\n",
    "    c = constant > 1\n",
    "    \n",
    "    OUTPUTS:\n",
    "    -------\n",
    "    s = indices that meet filter criteria\n",
    "    '''\n",
    "    \n",
    "    test = np.sum(tao)\n",
    "    \n",
    "    # Check if outlier scores meet variance criteria\n",
    "    if test <= sigma*c:\n",
    "        s = s\n",
    "    \n",
    "    else:\n",
    "        T = np.random.uniform(low=0, high=np.max(tao))\n",
    "        ind = np.where(tao < T)[0]\n",
    "    \n",
    "        # Take indices from previous candidate set\n",
    "        s = s[ind]\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def severRegTheory(x, y, FactorizedRegularizedCG, BlackBoxA, regGrad, lam, filt, sigma, c, gam, maxIters, eps, x0):\n",
    "    '''\n",
    "    Theoretical SEVER Algorithm for Sochastic Optimization (Regression)\n",
    "    \n",
    "    INPUTS:\n",
    "    -------\n",
    "    x = feature matrix\n",
    "    y = target vector\n",
    "    FactorizedRegularizedCG = ridge regression solver\n",
    "    BlackBoxA = function for computing LHS in RR problem\n",
    "    regGrad = function for computing gradient of objective\n",
    "    lam = regularization paramter (unscaled)\n",
    "    filt = filtration algorithm to select outliers\n",
    "    sigma = variance paramter\n",
    "    c = constant > 1\n",
    "    gam = regularization parameter (scaled)\n",
    "    maxIters = maximum iterations for solver\n",
    "    eps = error tolerance for solver\n",
    "    x0 = initial guess for w\n",
    "    \n",
    "    OUTPUTS:\n",
    "    -------\n",
    "    w = optimal parameter after outliers have been filtered\n",
    "    it = iterations of SEVER\n",
    "    t_s = run time\n",
    "    '''\n",
    "    \n",
    "    tstart = time.time()\n",
    "    \n",
    "    cs = x.shape[0]\n",
    "    s = np.array(range(cs))\n",
    "    s1 = np.array([])\n",
    "    x1 = x\n",
    "    y1 = y\n",
    "    it = 0\n",
    "    \n",
    "    # Find optimal solution using all data\n",
    "    [w, iters, t] = FactorizedRegularizedCG(BlackBoxA, x, y, gam, maxIters, eps, x0)\n",
    "    \n",
    "    while cs != len(s1):\n",
    "        \n",
    "        # Extract new indices\n",
    "        x = x1[s, :]\n",
    "        y = y1[s]\n",
    "        \n",
    "        # Find optimal solution from set of s points\n",
    "        [w, iters, t] = FactorizedRegularizedCG(BlackBoxA, x, y, gam, maxIters, eps, w)\n",
    "        \n",
    "        # Compute the gradient at s points\n",
    "        gradmat = regGrad(x, y, w, lam)\n",
    "        \n",
    "        # Form G matrix which is gradient at each point minus the average\n",
    "        # G has s rows (number of used indices) and d columns (dimension of data)\n",
    "        # Thus each row of G is grad_i-ave_grad\n",
    "        gradhat = (1/cs)*np.sum(gradmat, axis=1)\n",
    "        g = gradmat - np.array([gradhat,]*cs).transpose()\n",
    "        g = g.transpose()\n",
    "        \n",
    "        # Get largest right singluar vector in the 2-norm\n",
    "        [u, sig, v] = np.linalg.svd(g, full_matrices=True)\n",
    "        vh = v[0, :]\n",
    "        \n",
    "        # Compute outlier scores\n",
    "        tao = np.square(np.dot(g, vh))\n",
    "        s1 = s\n",
    "        s = filt(s1, tao, sigma, c)\n",
    "        cs = len(s)\n",
    "        \n",
    "        if cs == len(s1):\n",
    "            if np.sum(s == s1) == cs:\n",
    "                break\n",
    "        \n",
    "        # Increase iteration count\n",
    "        it = it + 1\n",
    "    \n",
    "    # Report run time of SEVER in milliseconds\n",
    "    tend = time.time()\n",
    "    t_s = (tend - tstart)*1000\n",
    "        \n",
    "    return [w, it, t_s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Practical SEVER (paper used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def filt(s, tao, p):\n",
    "    '''\n",
    "    Practical filter function for sever.\n",
    "    \n",
    "    INPUTS:\n",
    "    -------\n",
    "    s = set of indices\n",
    "    tao = vector of outlier scores\n",
    "    p = fraction of points to filter\n",
    "    \n",
    "    OUTPUTS:\n",
    "    -------\n",
    "    s = indices that meet filter criteria\n",
    "    '''\n",
    "\n",
    "    n = len(tao)\n",
    "    \n",
    "    # Compute fraction of outliers needed to discard\n",
    "    frac = math.floor(p*n)\n",
    "    \n",
    "    # Sort tao smallest to largest value\n",
    "    # This returns indices\n",
    "    tao = np.argsort(tao)\n",
    "    \n",
    "    # Discard high values\n",
    "    ind = tao[0:(n - frac)]\n",
    "    \n",
    "    # Take indices from previous candidate set\n",
    "    s = s[ind]\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def severReg(x, y, FactorizedRegularizedCG, BlackBoxA, regGrad, lam, filt, maxit, p, gam, maxIters, eps, x0):\n",
    "    '''\n",
    "    Practical SEVER Algorithm for Sochastic Optimization (Regression)\n",
    "    \n",
    "    INPUTS:\n",
    "    -------\n",
    "    x = feature matrix\n",
    "    y = target vector\n",
    "    FactorizedRegularizedCG = ridge regression solver\n",
    "    BlackBoxA = function for computing LHS in RR problem\n",
    "    regGrad = function for computing gradient of objective\n",
    "    lam = regularization paramter (unscaled)\n",
    "    filt = filtration algorithm to select outliers\n",
    "    maxit = max iterations for SEVER\n",
    "    p = fraction of points to throw out through filter\n",
    "    gam = regularization parameter (scaled)\n",
    "    maxIters = maximum iterations for solver\n",
    "    eps = error tolerance for solver\n",
    "    x0 = initial guess for w\n",
    "    \n",
    "    OUTPUTS:\n",
    "    -------\n",
    "    w = optimal parameter after outliers have been filtered\n",
    "    it = iterations of SEVER\n",
    "    t_s = run time\n",
    "    '''\n",
    "    \n",
    "    tstart = time.time()\n",
    "    \n",
    "    cs = x.shape[0]\n",
    "    s = np.array(range(cs))\n",
    "    x1 = x\n",
    "    y1 = y\n",
    "    it = 0\n",
    "    \n",
    "    # Find optimal solution using all data\n",
    "    [w, iters, t] = FactorizedRegularizedCG(BlackBoxA, x, y, gam, maxIters, eps, x0)\n",
    "    \n",
    "    while it < maxit:\n",
    "        \n",
    "        # Extract new indices\n",
    "        x = x1[s, :]\n",
    "        y = y1[s]\n",
    "        \n",
    "        # Find optimal solution from set of s points\n",
    "        [w, iters, t] = FactorizedRegularizedCG(BlackBoxA, x, y, gam, maxIters, eps, w)\n",
    "        \n",
    "        # Compute the gradient at s points\n",
    "        gradmat = regGrad(x, y, w, lam)\n",
    "        \n",
    "        # Form G matrix which is gradient at each point minus the average\n",
    "        # G has s rows (number of used indices) and d columns (dimension of data)\n",
    "        # Thus each row of G is grad_i-ave_grad\n",
    "        gradhat = (1/cs)*np.sum(gradmat, axis=1)\n",
    "        g = gradmat - np.array([gradhat,]*cs).transpose()\n",
    "        g = g.transpose()\n",
    "        \n",
    "        # Get largest right singluar vector in the 2-norm\n",
    "        [u, sig, v] = np.linalg.svd(g, full_matrices=True)\n",
    "        vh = v[0, :]\n",
    "        \n",
    "        # Compute outlier scores\n",
    "        tao = np.square(np.dot(g, vh))\n",
    "        s = filt(s, tao, p)\n",
    "        cs = len(s)\n",
    "        \n",
    "        if cs == 0:\n",
    "            print('You have chosen too many outliers to remove at each iteration.')\n",
    "            break\n",
    "        \n",
    "        # Increase iteration count\n",
    "        it = it + 1\n",
    "    \n",
    "    # Report run time of SEVER in milliseconds\n",
    "    tend = time.time()\n",
    "    t_s = (tend - tstart)*1000\n",
    "        \n",
    "    return [w, it, t_s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Numerical Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Set up to compare SEVER to CG method\n",
    "\n",
    "# Generate synthetic data as described in the paper\n",
    "n = 5000; t = 100; l = 500; w_true = np.random.uniform(low=0, high=2*math.pi, size=l)\n",
    "\n",
    "# Add details about attack data\n",
    "nb = math.floor(n*0.02)                         # number of data that is \"corrupted\"\n",
    "alp = np.random.uniform(low=0.001, high=100)    # hyperparameter to increase effectiveness of attack\n",
    "bet = np.random.uniform(low=0.001, high=100)    # hyperparameter to increase effectiveness of attack\n",
    "\n",
    "[x_train_reg, y_train_reg, x_test_reg, y_test_reg] = genRan(n, t, l, w_true, tog=0, nbad=nb, alpha=alp, beta=bet)\n",
    "\n",
    "# Specifics for CG methods\n",
    "lam = 0.001; gam = n*lam; maxIters = 50; eps = 10**(-12); x0 = np.zeros((500,))\n",
    "maxit = 4                                       # total number of iterations for practical SEVER\n",
    "sigma = 1000; c = 10                            # variance parameters for theoretical SEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Compare SEVER on test data to other methods:\n",
    "\n",
    "compnorm = la.norm(y_test_reg)\n",
    "# Find optimal w\n",
    "[w_sevtheory, sevtheory_it, t_sevtheory] = severRegTheory(x_train_reg, y_train_reg, FactorizedRegularizedCG, BlackBoxA, regGrad, lam, SEVERfilt, sigma, c, gam, maxIters, eps, x0)\n",
    "[w_sever, sever_it, t_sever] = severReg(x_train_reg, y_train_reg, FactorizedRegularizedCG, BlackBoxA, regGrad, lam, filt, maxit, (0.02)/2, gam, maxIters, eps, x0)\n",
    "[w_grad, grad_it, t_grad] = gradRemoval(x_train_reg, y_train_reg, FactorizedRegularizedCG, BlackBoxA, regGrad, lam, maxit, (0.02)/2, gam, maxIters, eps, x0)\n",
    "[w_cg, cg_it, t_cg] = FactorizedRegularizedCG(BlackBoxA, x_train_reg, y_train_reg, gam, maxIters*maxit, eps, x0)\n",
    "print('Practical SEVER runtime in ms: ', round(t_sever, 2), '\\nTheoretical SEVER runtime in ms: ', round(t_sevtheory, 2), '\\nGradient Centered Removal runtime in ms: ', round(t_grad, 2), '\\nCG runtime in ms: ', round(t_cg, 2))\n",
    "\n",
    "# Use w^* on uncorrupted testing data\n",
    "sever_error = (1/t)*la.norm(np.dot(x_test_reg, w_sever) - y_test_reg)/compnorm\n",
    "severtheory_error = (1/t)*la.norm(np.dot(x_test_reg, w_sevtheory) - y_test_reg)/compnorm\n",
    "grad_error = (1/t)*la.norm(np.dot(x_test_reg, w_grad) - y_test_reg)/compnorm\n",
    "cg_error = (1/t)*la.norm(np.dot(x_test_reg, w_cg) - y_test_reg)/compnorm\n",
    "print('Relative Test Error for Practical SEVER: ', round(sever_error, 6),'\\nRelative Test Error for Theoretical SEVER: ', round(severtheory_error, 6),'\\nRelative Test Error for Gradient Centered l-2 Removal: ', round(grad_error, 6), '\\nRelative Test Error for Regular Ridge Regression with CG: ', round(cg_error, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Compare SEVER over a range of attack data\n",
    "\n",
    "compnorm = la.norm(y_test_reg)\n",
    "\n",
    "# Create vector to test\n",
    "nbvec = np.array([0, int(0.01*n), int(0.02*n), int(0.03*n), int(0.04*n), int(0.05*n), int(0.06*n), int(0.07*n), int(0.08*n), int(0.09*n), int(0.1*n)])\n",
    "\n",
    "Err_sever = np.array([]); Err_severtheory = np.array([]); Err_cg = np.array([]); Err_grad = np.array([])\n",
    "time_sever = np.array([]); time_severtheory = np.array([]); time_cg = np.array([]); time_grad = np.array([])\n",
    "\n",
    "for i in range(len(nbvec)):\n",
    "    # Generate data\n",
    "    [x_train_reg, y_train_reg, x_test_reg, y_test_reg] = genRan(n, t, l, w_true, tog=0, nbad=nbvec[i], alpha=alp, beta=bet)\n",
    "    \n",
    "    # Run practical SEVER\n",
    "    [w_sever, sever_it, t_sever] = severReg(x_train_reg, y_train_reg, FactorizedRegularizedCG, BlackBoxA, regGrad, lam, filt, maxit, (nbvec[i])/(n*2), gam, maxIters, eps, x0)\n",
    "    \n",
    "    # Run theoretical SEVER\n",
    "    [w_sevtheory, sevtheory_it, t_sevtheory] = severRegTheory(x_train_reg, y_train_reg, FactorizedRegularizedCG, BlackBoxA, regGrad, lam, SEVERfilt, sigma, c, gam, maxIters, eps, x0)\n",
    "    \n",
    "    # Run gradient centered removal\n",
    "    [w_grad, grad_it, t_grad] = gradRemoval(x_train_reg, y_train_reg, FactorizedRegularizedCG, BlackBoxA, regGrad, lam, maxit, (nbvec[i])/(n*2), gam, maxIters, eps, x0)\n",
    "    \n",
    "    # Compute Ridge Regression solution\n",
    "    [w_cg, it_cg, t_cg] = FactorizedRegularizedCG(BlackBoxA, x_train_reg, y_train_reg, gam, maxIters*maxit, eps, x0)\n",
    "    \n",
    "    # Compute testing errors\n",
    "    sever_error = (1/t)*la.norm(np.dot(x_test_reg, w_sever) - y_test_reg)/compnorm\n",
    "    severtheory_error = (1/t)*la.norm(np.dot(x_test_reg, w_sevtheory) - y_test_reg)/compnorm\n",
    "    grad_error = (1/t)*la.norm(np.dot(x_test_reg, w_grad) - y_test_reg)/compnorm\n",
    "    cg_error = (1/t)*la.norm(np.dot(x_test_reg, w_cg) - y_test_reg)/compnorm\n",
    "    \n",
    "    # Record testing errors\n",
    "    Err_sever = np.append(Err_sever, sever_error)\n",
    "    Err_severtheory = np.append(Err_severtheory, severtheory_error)\n",
    "    Err_grad = np.append(Err_grad, grad_error)\n",
    "    Err_cg = np.append(Err_cg, cg_error)\n",
    "    \n",
    "    # Record time\n",
    "    time_sever = np.append(time_sever, t_sever)\n",
    "    time_severtheory = np.append(time_severtheory, t_sevtheory)\n",
    "    time_grad = np.append(time_grad, t_grad)\n",
    "    time_cg = np.append(time_cg, t_cg)\n",
    "\n",
    "# Ensure dimensions are appropriate for plotting\n",
    "Err_sever = Err_sever.flatten()\n",
    "Err_severtheory = Err_severtheory.flatten()\n",
    "Err_grad = Err_grad.flatten()\n",
    "Err_cg = Err_cg.flatten()\n",
    "\n",
    "time_sever = time_sever.flatten()\n",
    "time_severtheory = time_severtheory.flatten()\n",
    "time_grad = time_grad.flatten()\n",
    "time_cg = time_cg.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Plots of SEVER test error over a range of attacks\n",
    "\n",
    "xax = np.linspace(nbvec[0], nbvec[len(nbvec)-1], len(nbvec), endpoint=True); xax = (xax/n)\n",
    "plt.plot(xax, Err_sever, 'b-', label='PracSEVER')\n",
    "plt.plot(xax, Err_cg, 'r-', label='CG')\n",
    "plt.plot(xax, Err_severtheory, 'k-', label='TheorySEVER')\n",
    "plt.plot(xax, Err_grad, 'g-', label='GradCenter')\n",
    "plt.title('Synthetic Data Error')\n",
    "plt.ylabel('RMSE/$||y||_2$')\n",
    "plt.xlabel('Percent of Corrupted Training Points')\n",
    "plt.legend()\n",
    "# plt.savefig('regressionSEVER.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Compare runtime of different algorithms\n",
    "\n",
    "plt.plot(xax, time_sever, 'b-', label='PracSEVER')\n",
    "plt.plot(xax, time_cg, 'r-', label='CG')\n",
    "plt.plot(xax, time_severtheory, 'k-', label='TheorySEVER')\n",
    "plt.plot(xax, time_grad, 'g-', label='GradCenter')\n",
    "plt.title('Algorithm Run Time')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.xlabel('Percent of Corrupted Training Points')\n",
    "plt.legend()\n",
    "# plt.savefig('regressionSEVERtime.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Comparison of SEVER Across Corruption Amounts\n",
    "---\n",
    "- Synthetic data has percent outliers $\\varepsilon\\in\\{0.01,0.02,\\dots,0.1\\}$\n",
    "- For `PracSEVER` and `GradCenter` we remove $p=\\frac{\\varepsilon\\cdot n}{2}$ points\n",
    "<table><tr><td><img src='regressionSEVER.png'></td><td><img src='regressionSEVERtime.png'></td></tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Song Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Year prediction data\n",
    "\n",
    "# Choose number of training and testing data\n",
    "numtrain = 5000\n",
    "numtest = 200\n",
    "\n",
    "# Training data\n",
    "df_train = pd.read_csv('YearPredictionMSD.txt', header=None, nrows=numtrain)\n",
    "df_train = pd.DataFrame(df_train)\n",
    "y_train = df_train[0]\n",
    "ytrain = np.array(y_train)\n",
    "X_train = df_train.drop(columns=[0])\n",
    "xtrain = np.array(X_train)\n",
    "\n",
    "# Testing data\n",
    "df_test = pd.read_csv('YearPredictionMSD.txt', header=None, skiprows=463715, nrows=numtest)\n",
    "df_test = pd.DataFrame(df_test)\n",
    "y_test = df_test[0]\n",
    "ytest = np.array(y_test)\n",
    "X_test = df_test.drop(columns=[0])\n",
    "xtest = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Corrupt training data\n",
    "pcor = 0.1\n",
    "[xtrain_c, ytrain_c] = dataAttack(xtrain, ytrain, pb=pcor, alpha=-np.random.uniform(0.001, 50), beta=np.random.uniform(1900, 2100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "maxiter = 3; x0 = np.zeros((90,)); eps = 10**(-5); cgiters = 100; lam = 0.01; gamma = numtrain*lam\n",
    "\n",
    "# Preprocess data\n",
    "xtrainnew = xtrain - np.mean(xtrain, axis=0)\n",
    "xtestnew = xtest - np.mean(xtest, axis=0)\n",
    "ytrainnew = ytrain - np.mean(ytrain)\n",
    "ytestnew = ytest - np.mean(ytest)\n",
    "normcomp2 = la.norm(ytestnew)\n",
    "xtrainnewc = xtrain_c - np.mean(xtrain_c, axis=0)\n",
    "ytrainnewc = ytrain_c - np.mean(ytrain_c)\n",
    "\n",
    "# Practical SEVER without corruptions\n",
    "[w_yp_p, it_yp_p, t_yp_p] = severReg(xtrainnew, ytrainnew, FactorizedRegularizedCG, BlackBoxA, regGrad, lam, filt, maxiter, (pcor/2), gamma, cgiters, eps, x0)\n",
    "yp_error_p = (1/numtest)*la.norm(np.dot(xtestnew, w_yp_p) - ytestnew)/normcomp2\n",
    "print('SEVER RMSE without corruptions: ',round(yp_error_p,6))\n",
    "\n",
    "# Regular Conjugate Gradient without corruptions\n",
    "[w_yp_cg, it_yp_cg, t_yp_cg] = FactorizedRegularizedCG(BlackBoxA, xtrainnew, ytrainnew, gamma, maxiter*cgiters, eps, x0)\n",
    "yp_error_cg = (1/numtest)*la.norm(np.dot(xtestnew, w_yp_cg) - ytestnew)/normcomp2\n",
    "print('CG RMSE without corruptions: ',round(yp_error_cg,6))\n",
    "\n",
    "maxiter = 2;\n",
    "# Practical SEVER with corruptions\n",
    "[w_yp_p_c, it_yp_p_c, t_yp_p_c] = severReg(xtrainnewc, ytrainnewc, FactorizedRegularizedCG, BlackBoxA, regGrad, lam, filt, maxiter, (pcor)/2, gamma, cgiters, eps, x0)\n",
    "yp_error_p_c = (1/numtest)*la.norm(np.dot(xtestnew, w_yp_p_c) - ytestnew)/normcomp2\n",
    "print('SEVER RMSE with corruptions: ',round(yp_error_p_c,6))\n",
    "\n",
    "# Regular Conjugate Gradient with corruptions\n",
    "[w_yp_cg_c, it_yp_cg_c, t_yp_cg_c] = FactorizedRegularizedCG(BlackBoxA, xtrainnewc, ytrainnewc, gamma, maxiter*cgiters, eps, x0)\n",
    "yp_error_cg_c = (1/numtest)*la.norm(np.dot(xtestnew, w_yp_cg_c) - ytestnew)/normcomp2\n",
    "print('CG RMSE with corruptions: ',round(yp_error_cg_c,6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# SEVER Comparison on Real Data\n",
    "---\n",
    "- Use \"Year Prediction Data\"\n",
    "- Test on original data and on corrupted data\n",
    "- Center target vector and data\n",
    "- Use `PracSEVER` only\n",
    "\n",
    "|Method|Corrupt?|RMSE/$||y||_2$|\n",
    "|---|---|---|\n",
    "|`PracSEVER`|No|0.004076|\n",
    "|`CG`|No|0.004065|\n",
    "|`PracSEVER`|Yes|0.006808|\n",
    "|`CG`|Yes|0.006731|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SEVER + Clustering?\n",
    "---\n",
    "- Does this type of filtering extend to other machine learning methods?\n",
    "- Use `Iris` dataset \n",
    "- Use K-means with cluster set at 3\n",
    "- Use the **practical** filtering method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Numerical Results\n",
    "---\n",
    "- Toy example: just use the `Sepal Width` and `Pedal Width` features\n",
    "<table><tr><td><img src='Iris2d.png'></td><td>Initial Training Accuracy: 4%<br>Initial Testing Accuracy: 4%<br>Training Accuracy after SEVER: 37%<br>Testing Accuracy after SEVER: 44%</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Using the Entire Iris Dataset\n",
    "---\n",
    "- Run `SEVER` 50 times\n",
    "- Filter out 2% of the data every time\n",
    "\n",
    "|`SEVER`?|Training Accuracy|Testing Accuracy|\n",
    "|---|---|---|\n",
    "|No|87%|90%|\n",
    "|Yes|85%|92%|\n",
    "\n",
    "Note: runtime is about 1 second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Clustering Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as km\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = datasets.load_iris()\n",
    "xiris = iris.data[:, (1,3)]\n",
    "xirisfull = iris.data\n",
    "yiris = iris.target\n",
    "\n",
    "# Separate into training and testing data\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(xiris, yiris, test_size=0.33, random_state=42)\n",
    "xtrainfull, xtestfull, ytrainfull, ytestfull = train_test_split(xirisfull, yiris, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def SEVERCluster(x, y, xt, yt, kmnum, filt, p, maxit):\n",
    "    '''\n",
    "    SEVER for Clustering\n",
    "    \n",
    "    INPUTS:\n",
    "    -------\n",
    "    x = training features\n",
    "    y = training targets\n",
    "    xt = testing features\n",
    "    yt = testing targets\n",
    "    kmnum = number of clusters\n",
    "    filt = filtering algorithm\n",
    "    p = percent of points thrown out each time\n",
    "    maxit = number of iterations of SEVER\n",
    "    \n",
    "    OUTPUTS:\n",
    "    -------\n",
    "    ftrain_acc = final training accuracy\n",
    "    ftest_acc = final testing accuracy\n",
    "    t_s = run time of algorithm\n",
    "    xf = final or 'reduced' feature matrix\n",
    "    fcentroids = final centroids\n",
    "    '''\n",
    "    \n",
    "    tstart = time.time()\n",
    "    \n",
    "    cs = x.shape[0]\n",
    "    s = np.array(range(cs))\n",
    "    x1 = np.copy(x)\n",
    "    y1 = np.copy(y)\n",
    "    \n",
    "    # Precompute for efficiency\n",
    "    gradx = np.sum(x, axis=0)\n",
    "    \n",
    "    # Test on unaltered data\n",
    "    kmeans = km(n_clusters=kmnum, random_state=0).fit(x)\n",
    "    print('Initial training accuracy: ',sum(kmeans.labels_==y)/len(y))\n",
    "    print('Initial testing accuracy: ',sum(kmeans.predict(xt)==yt)/len(yt))\n",
    "    \n",
    "    it = 0\n",
    "    \n",
    "    while it <= maxit:\n",
    "        \n",
    "        # Extract new indices\n",
    "        x = x1[s, :]\n",
    "        y = y1[s]\n",
    "        \n",
    "        # Cluster on x[s]\n",
    "        kmeans = km(n_clusters=kmnum, random_state=0).fit(x)\n",
    "        \n",
    "        centroids = kmeans.cluster_centers_\n",
    "        \n",
    "        # Find which center is closest to the data point\n",
    "        norms = np.zeros((x.shape[0], kmnum))\n",
    "        \n",
    "        centroids = kmeans.cluster_centers_\n",
    "        \n",
    "        for i in range(kmnum):\n",
    "            norms[:,i] = la.norm(x - centroids[i,:], axis=1)\n",
    "            \n",
    "        mins = np.argmin(norms, axis=1)\n",
    "        \n",
    "        gradmat = x - centroids[mins]\n",
    "        \n",
    "        # Form G matrix which is gradient at each point minus the average\n",
    "        # G has s rows (number of used indices) and d columns (dimension of data)\n",
    "        # Thus each row of G is grad_i-ave_grad\n",
    "        gradhat = (1/cs)*np.sum(gradmat, axis=0)\n",
    "        g = gradmat - np.array([gradhat,]*cs)\n",
    "        g = g\n",
    "        \n",
    "        # Get largest right singluar vector in the 2-norm\n",
    "        [u, sig, v] = np.linalg.svd(g, full_matrices=True)\n",
    "        vh = v[0, :]\n",
    "        \n",
    "        # Compute outlier scores\n",
    "        tao = np.square(np.dot(g, vh))\n",
    "        s = filt(s, tao, p)\n",
    "        cs = len(s)\n",
    "        \n",
    "        if cs == 0:\n",
    "            print('You have chosen too many outliers to remove at each iteration.')\n",
    "            break\n",
    "         \n",
    "        it = it + 1\n",
    "        \n",
    "    # Save reduced x\n",
    "    xf = np.copy(x)\n",
    "    \n",
    "    # Report run time of SEVER in milliseconds\n",
    "    tend = time.time()\n",
    "    t_s = (tend - tstart)*1000\n",
    "    \n",
    "    # Test on testing data\n",
    "    ftrain_acc = sum(kmeans.labels_==y)/len(y)\n",
    "    ftest_acc = sum(kmeans.predict(xt)==yt)/len(yt)\n",
    "    fcentroids = kmeans.cluster_centers_\n",
    "    \n",
    "    return [ftrain_acc, ftest_acc, t_s, xf, fcentroids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Use only part of the data\n",
    "\n",
    "[acctrain, acctest, tim, xtrain_red, centers] = SEVERCluster(xtrain, ytrain, xtest, ytest, 3, filt, 0.02, 10)\n",
    "#print(acctrain)\n",
    "#print(acctest)\n",
    "plt.scatter(xtrain[:,0], xtrain[:,1], facecolors='none', color='b', linewidth=0.5, label='BeforeSEVER')\n",
    "plt.scatter(xtrain_red[:,0], xtrain_red[:,1], marker=\"+\", color='r', label='AfterSEVER')\n",
    "plt.scatter(centers[:,0], centers[:,1], color='k', marker=\"*\", label='Centers')\n",
    "plt.title('Iris Data Points before and after SEVER')\n",
    "plt.ylabel('Pedal Width')\n",
    "plt.xlabel('Sepal Width')\n",
    "plt.legend()\n",
    "#plt.savefig('Iris2d.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Use all of the data\n",
    "\n",
    "[acctrain, acctest, tim, xtrain_red, centers] = SEVERCluster(xtrainfull, ytrainfull, xtestfull, ytestfull, 3, filt, 0.02, 50)\n",
    "print(acctrain)\n",
    "print(acctest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "---\n",
    "|Pros|Cons|\n",
    "|---|---|\n",
    "|Relatively easily implemented|Potentially slow runtime|\n",
    "|*Usually* gets better results|Potentially at the cost of tuning hyperparamters|\n",
    "|Generalizes to other problem settings|Data cannot be coupled|\n",
    "|Has theoretical backing|Some issues in the proofs|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Thank You"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
